{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f73b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "#from capsnet import CapsNet\n",
    "#from data_loader import Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import combinations, permutations\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import itertools\n",
    "import matplotlib.cm as cm\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "from shutil import copyfile\n",
    "from functools import partial\n",
    "\n",
    "from scipy.special import gammaln\n",
    "\n",
    "#import pathos.multiprocessing as mp\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "#import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "#import pathos.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "import pathos.multiprocessing as mp\n",
    "\n",
    "import sys\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance(s, b):\n",
    "    if(b!=0):\n",
    "        square_s = s/math.sqrt(b)\n",
    "        if(square_s>0):\n",
    "            return square_s\n",
    "        else: return 0\n",
    "    if(b==0 and s==0):\n",
    "        return 0\n",
    "    if(b==0 and s!=0):\n",
    "        print (\" a strange case\")\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0399ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel=1, kin_features=21, num_classes=2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channel, 32, kernel_size=(3,3,3), stride=(1,1,1), padding=(0,1,1), bias=True),\n",
    "            nn.BatchNorm3d(32),\n",
    "            #nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=(3,3), stride=(1,1), padding=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            #nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=(3,3), stride=(2,2), padding=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            #nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        #self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.fc1 = nn.Linear(32*4*4, 1500)\n",
    "        #self.fc2 = nn.Linear(1500+600, num_classes)\n",
    "        \n",
    "        self.kin_fc = nn.Sequential(\n",
    "            nn.Linear(kin_features, 600),\n",
    "            #nn.BatchNorm1d(2**8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 600),\n",
    "            nn.ReLU(),\n",
    "            #nn.Tanh()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1500+600, 600),\n",
    "            #nn.BatchNorm1d(2**8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, num_classes)\n",
    "            #nn.ReLU(),\n",
    "            #nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        c1 = self.conv1(x.unsqueeze(1))\n",
    "        c1 = self.relu(c1)\n",
    "        c1 = c1.squeeze()\n",
    "        c2 = self.conv2(c1)\n",
    "        c2 = self.relu(c2)\n",
    "        c3 = self.conv2(c2)\n",
    "        b2 = self.conv2(c1)\n",
    "        b2 = self.relu(b2)\n",
    "        b3 = self.conv2(b2)\n",
    "        b3 = self.relu(b3)\n",
    "        b4 = self.conv2(b3)\n",
    "        #print (c1.size(), c3.size(), b4.size())\n",
    "        c10 = c1+c3+b4\n",
    "        c10 = self.relu(c10)\n",
    "        \n",
    "        c11 = self.conv3(c10)\n",
    "        c11 = self.relu(c11)\n",
    "        c12 = self.conv2(c11)\n",
    "        c13 = self.conv3(c10)\n",
    "        c14 = c12+c13\n",
    "        c14 = self.relu(c14)\n",
    "        \n",
    "        c15 = self.conv2(c14)\n",
    "        c15 = self.relu(c15)\n",
    "        c16 = self.conv2(c15)\n",
    "        c17 = c14+c16\n",
    "        c17 = self.relu(c17)\n",
    "        \n",
    "        c18 = self.conv2(c17)\n",
    "        c18 = self.relu(c18)\n",
    "        c19 = self.conv2(c18)\n",
    "        c20 = c17+c19\n",
    "        c20 = self.relu(c20)\n",
    "        \n",
    "        c21 = self.conv3(c20)\n",
    "        c21 = self.relu(c21)\n",
    "        c22 = self.conv2(c21)\n",
    "        c23 = self.conv3(c20)\n",
    "        c24 = c22+c23\n",
    "        c24 = self.relu(c24)\n",
    "        \n",
    "        c25 = self.conv2(c24)\n",
    "        c25 = self.relu(c25)\n",
    "        c26 = self.conv2(c25)\n",
    "        c27 = c24+c26\n",
    "        c27 = self.relu(c27)\n",
    "        \n",
    "        c28 = self.conv2(c27)\n",
    "        c28 = self.relu(c28)\n",
    "        c29 = self.conv2(c28)\n",
    "        c30 = c27+c29\n",
    "        c30 = self.relu(c30)\n",
    "\n",
    "        c31 = self.conv3(c30)\n",
    "        c31 = self.relu(c31)\n",
    "        c32 = self.conv2(c31)\n",
    "        c33 = self.conv3(c30)\n",
    "        c34 = c32+c33\n",
    "        c34 = self.relu(c34)\n",
    "        \n",
    "        c35 = self.conv2(c34)\n",
    "        c35 = self.relu(c35)\n",
    "        c36 = self.conv2(c35)\n",
    "        c37 = c34+c36\n",
    "        c37 = self.relu(c37)\n",
    "\n",
    "        c38 = self.conv2(c37)\n",
    "        c38 = self.relu(c38)\n",
    "        c39 = self.conv2(c38)\n",
    "        c40 = c37+c39\n",
    "        c40 = self.relu(c40)\n",
    "\n",
    "        c41 = self.conv3(c40)\n",
    "        c41 = self.relu(c41)\n",
    "        c42 = self.conv2(c41)\n",
    "        c43 = self.conv3(c40)\n",
    "        c44 = c42+c43\n",
    "        c44 = self.relu(c44)\n",
    "    \n",
    "        return c44\n",
    "\n",
    "    def forward(self, x, data):\n",
    "        x = self._forward_impl(x)\n",
    "        #print (\"x\", x.size())\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        data = self.kin_fc(data)\n",
    "        out = torch.cat((x, data), dim=1)\n",
    "        out = self.fc2(out)\n",
    "        return F.softmax(out)\n",
    "        \n",
    "        #return self._forward_impl(x, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1397e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data = np.load(\"./v13_combo_train.npy\").astype(np.float16)\n",
    "test_data = np.load(\"./v13_combo_test.npy\").astype(np.float16)\n",
    "train_data_V2 = np.load(\"./v13_kinematic_combo_train.npy\").astype(np.float32)\n",
    "test_data_V2 = np.load(\"./v13_kinematic_combo_test.npy\").astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cf126",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.concatenate((np.zeros(190000), np.ones(99000), np.ones(55000), np.ones(2000), np.ones(2500), np.ones(10000), np.ones(600))).astype(np.int)\n",
    "Y_test = np.concatenate((np.zeros(50000), np.ones(190000), np.ones(8000), np.ones(1000), np.ones(1000), np.ones(2400), np.ones(100))).astype(np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe05e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM=0\n",
    "TRAIN_ON_GPU = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "cuda = torch.device(f\"cuda:{GPU_NUM}\" if TRAIN_ON_GPU else \"cpu\")\n",
    "torch.cuda.set_device(cuda)\n",
    "\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d233de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "idx = np.arange(len(train_data))\n",
    "np.random.shuffle(idx)\n",
    "train_data, Y_train = train_data[idx], Y_train[idx]\n",
    "train_data_V2 = train_data_V2[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "cnn = ResNet(in_channel=1, kin_features=21, num_classes=2).float().cuda()\n",
    "\n",
    "cnn = nn.DataParallel(cnn)\n",
    "\n",
    "#optimizer = torch.optim.Adam(capsule_net.parameters())\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "result = []\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1.0, 1.0]).float()).cuda()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=math.exp(-0.0))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=math.exp(-0.01))  \n",
    "rl_ = []\n",
    "# Train the Model\n",
    "rloss_, eloss_ = [], []\n",
    "racc_, eacc_ = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    #np.random.seed(0)\n",
    "    idx = np.arange(len(train_data))\n",
    "    np.random.shuffle(idx)\n",
    "    x_train, y_train = train_data[idx], Y_train[idx]\n",
    "    x_train_V2 = train_data_V2[idx]\n",
    "\n",
    "    x_train_split = np.array_split(x_train, 500)\n",
    "    x_train_V2_split = np.array_split(x_train_V2, 500)\n",
    "    y_train_split = np.array_split(y_train, 500)\n",
    "\n",
    "    x_test_split = np.array_split(test_data, 500)\n",
    "    x_test_V2_split = np.array_split(test_data_V2, 500)\n",
    "    y_test_split = np.array_split(Y_test, 500)\n",
    "\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for i in range(len(x_train_split)):\n",
    "        x_train = x_train_split[i]\n",
    "        x_train_V2 = x_train_V2_split[i]\n",
    "        y_train = y_train_split[i]\n",
    "        x_train = Variable(torch.from_numpy(x_train)).float().cuda()\n",
    "        x_train_V2 = Variable(torch.from_numpy(x_train_V2)).float().cuda()\n",
    "        y_train = Variable(torch.from_numpy(y_train)).long().cuda()\n",
    "\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(x_train, x_train_V2)\n",
    "        pred = outputs.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct = pred.eq(y_train.data.view_as(pred)).long().sum().item()\n",
    "        train_correct+=correct\n",
    "\n",
    "        loss = criterion(outputs, y_train)\n",
    "        #print (i, loss.data.item())\n",
    "        train_loss+=loss.data.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    rl_.append(scheduler.get_lr()[0])\n",
    "    scheduler.step()\n",
    "\n",
    "    #if(epoch<=10 or epoch%10==0):\n",
    "    if(True):\n",
    "        torch.cuda.empty_cache() #release some memory\n",
    "        cnn = cnn.eval()\n",
    "        \n",
    "        test_result = []\n",
    "        test_loss, test_correct = 0, 0\n",
    "        for i in range(len(x_test_split)):\n",
    "            x_test = x_test_split[i]\n",
    "            x_test_V2 = x_test_V2_split[i]\n",
    "            y_test = y_test_split[i]\n",
    "            x_test = Variable(torch.from_numpy(x_test)).float().cuda()\n",
    "            x_test_V2 = Variable(torch.from_numpy(x_test_V2)).float().cuda()\n",
    "            y_test = Variable(torch.from_numpy(y_test)).long().cuda()\n",
    "\n",
    "            outputs = cnn(x_test, x_test_V2)\n",
    "            pred = outputs.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct = pred.eq(y_test.data.view_as(pred)).long().sum().item()\n",
    "            test_correct+=correct\n",
    "\n",
    "            loss = criterion(outputs, y_test)\n",
    "            test_loss+=loss.data.item()\n",
    "            \n",
    "            test_result.append(outputs[:,0].detach().cpu().numpy().reshape(1, -1)[0])\n",
    "            \n",
    "        cnn = cnn.train()\n",
    "        \n",
    "        test_result = np.concatenate((test_result))\n",
    "        #signal, tt, tw = test_result[:50000], test_result[50000:50000+170000], test_result[50000+170000:]\n",
    "        np.save(\"./test_result\"+str(epoch), test_result)\n",
    "        \n",
    "        plt.hist(test_result[:50000], bins=np.linspace(0.0, 1.0, 100), color='r', label=\"Signal\", density=True, alpha=0.4)\n",
    "        plt.hist(test_result[50000:50000+190000], bins=np.linspace(0.0, 1.0, 100), color='b', label=\"tt\", density=True, alpha=0.4)\n",
    "        plt.hist(test_result[50000+190000:50000+190000+8000], bins=np.linspace(0.0, 1.0, 100), label=\"tw\", density=True, alpha=0.4)\n",
    "        plt.hist(test_result[50000+190000+8000:50000+190000+8000+1000], bins=np.linspace(0.0, 1.0, 100), label=\"tth\", density=True, alpha=0.4)\n",
    "        plt.hist(test_result[50000+190000+8000+1000:50000+190000+8000+1000+1000], bins=np.linspace(0.0, 1.0, 100), label=\"ttv\", density=True, alpha=0.4)\n",
    "        plt.hist(test_result[50000+190000+8000+1000+1000:50000+190000+8000+1000+1000+2400], bins=np.linspace(0.0, 1.0, 100), label=\"llbj\", density=True, alpha=0.4)\n",
    "        plt.hist(test_result[50000+190000+8000+1000+1000+2400:50000+190000+8000+1000+1000+2400+100], bins=np.linspace(0.0, 1.0, 100), label=\"tatabb\", density=True, alpha=0.4)\n",
    "        plt.xlabel(\"NN Output\", fontsize=15)\n",
    "        plt.ylabel(\"Number\", fontsize=15)\n",
    "        plt.legend(loc='best', fontsize=15)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        signal, ax = np.histogram(test_result[:50000], bins=np.linspace(0.99999, 1.0, 10000))\n",
    "        tt, _ = np.histogram(test_result[50000:50000+190000], bins=np.linspace(0.99999, 1.0, 10000))\n",
    "        tw, _ = np.histogram(test_result[50000+190000:50000+190000+8000], bins=np.linspace(0.99999, 1.0, 10000))\n",
    "        tth, _ = np.histogram(test_result[50000+190000+8000:50000+190000+8000+1000], bins=np.linspace(0.99999, 1.0, 10000))\n",
    "        ttv, _ = np.histogram(test_result[50000+190000+8000+1000:50000+190000+8000+1000+1000], bins=np.linspace(0.99999, 1.0, 10000))\n",
    "        llbj, _ = np.histogram(test_result[50000+190000+8000+1000+1000:50000+190000+8000+1000+1000+2400], bins=np.linspace(0.99999, 1.0, 10000))\n",
    "        tatabb, _ = np.histogram(test_result[50000+190000+8000+1000+1000+2400:50000+190000+8000+1000+1000+2400+100], bins=np.linspace(0.99999, 1.0, 10000))\n",
    "\n",
    "        signal, tt, tw, tth, ttv, llbj, tatabb = signal*3e3*0.0214964/50000, tt*3e3*120.907/190000, tw*3e3*4.38354/8000, tth*3e3*0.15258/1000, ttv*3e3*0.157968/1000, llbj*3e3*1.22936/2400, tatabb*3e3*0.011392/100\n",
    "        \n",
    "        significance_ = [significance(sum(signal[idx:]), sum(tt[idx:])+sum(tw[idx:])+sum(tth[idx:])+sum(ttv[idx:])+sum(llbj[idx:])+sum(tatabb[idx:])) for idx in range(len(signal))]\n",
    "        sig_number = [sum(signal[idx:]) for idx in range(len(signal))]\n",
    "        \n",
    "         \n",
    "        plt.plot(sig_number, significance_, color='r', label=\"Significance\")\n",
    "        plt.xlabel(\"Signal Number\", fontsize=15)\n",
    "        plt.ylabel(\"Significance\", fontsize=15)\n",
    "        plt.legend(loc='best', fontsize=15)\n",
    "        plt.show()\n",
    "\n",
    "        print (\"Epoch: %d, Time: %.2f, Lr: %.5f, Max Sig: %.6f, signal: %.2f, tt: %.2f, tw: %.2f, Train Loss: %.4f\" %(epoch, (time.time()-start_time)/3600., scheduler.get_lr()[0], np.max(significance_), sum(signal[np.argmax(significance_):]), sum(tt[np.argmax(significance_):]), sum(tw[np.argmax(significance_):]), train_loss/len(x_train_split)))\n",
    "        print (\"Train Correct: %d, Test Correct: %d, Train Loss: %.4f, Test Loss: %.4f\" %(train_correct, test_correct, train_loss/2000., test_loss/2000.))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ed177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e058a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
